{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792f3515",
   "metadata": {},
   "source": [
    "# ğŸ“Š Week 5: Data Wrangling and Aggregation\n",
    "\n",
    "## Welcome to Your Data Transformation Journey!\n",
    "\n",
    "**Duration:** ~2 hours  \n",
    "**Skill Progression:** ğŸŸ¢ Novice â†’ ğŸŸ¡ Intermediate â†’ ğŸ”´ Advanced\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "1. **Select** specific data subsets using `.loc[]`, `.iloc[]`, and boolean filtering\n",
    "2. **Aggregate** data using `groupby()` and multiple aggregation functions\n",
    "3. **Merge** multiple DataFrames using different join types\n",
    "4. **Transform** columns using operators, `.map()`, and custom functions\n",
    "5. **Visualize** your aggregated and transformed data\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Table of Contents\n",
    "\n",
    "| Section | Topic | Duration |\n",
    "|---------|-------|----------|\n",
    "| 1 | Setup & Warm-Up | 5 min |\n",
    "| 2 | Data Selection Techniques | 20 min |\n",
    "| 3 | Data Aggregation with GroupBy | 20 min |\n",
    "| 4 | Merging DataFrames | 20 min |\n",
    "| 5 | Data Transformation | 15 min |\n",
    "| 6 | Comprehensive Practice | 30 min |\n",
    "| 7 | Wrap-Up & Assignment Preview | 10 min |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¨ Skill Level Legend\n",
    "\n",
    "- ğŸŸ¢ **Novice**: Fundamental concepts - simple, clear, step-by-step\n",
    "- ğŸŸ¡ **Intermediate**: Building complexity - combining techniques\n",
    "- ğŸ”´ **Advanced**: Real-world patterns - production-ready approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758680f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Section 1: Setup & Environment Configuration\n",
    "\n",
    "Let's start by importing our essential libraries and configuring our visualization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9181d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: SETUP & ENVIRONMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "This cell imports all necessary libraries and configures visualization settings.\n",
    "Run this cell first before executing any other code in this notebook.\n",
    "\n",
    "Libraries Used:\n",
    "- pandas: Data manipulation and analysis\n",
    "- numpy: Numerical operations\n",
    "- matplotlib: Static visualizations\n",
    "- seaborn: Statistical visualizations (built on matplotlib)\n",
    "\"\"\"\n",
    "\n",
    "# Core data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure pandas display options for better readability\n",
    "pd.set_option('display.max_columns', 15)      # Show more columns\n",
    "pd.set_option('display.max_rows', 20)         # Show more rows\n",
    "pd.set_option('display.width', 120)           # Wider display\n",
    "pd.set_option('display.precision', 2)         # 2 decimal places\n",
    "\n",
    "# Configure matplotlib for cleaner visualizations\n",
    "plt.rcParams['figure.figsize'] = [10, 6]      # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 100              # Higher resolution\n",
    "plt.rcParams['font.size'] = 11                # Readable font size\n",
    "\n",
    "# Set seaborn style for beautiful plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings for cleaner output (in production, review warnings!)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries loaded successfully!\")\n",
    "print(f\"ğŸ“¦ Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ“¦ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749beb9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¬ Warm-Up Discussion\n",
    "\n",
    "**Before we dive in, let's discuss:**\n",
    "\n",
    "1. ğŸ¤” \"What's been the most challenging part of working with Pandas so far?\"\n",
    "2. ğŸ“Š \"When you look at a spreadsheet or table, how do you find specific information?\"\n",
    "3. ğŸ”— \"What about combining information from two different tables?\"\n",
    "\n",
    "**Real-World Connections:**\n",
    "- Finding all customers over age 30 in a database\n",
    "- Calculating average sales by region\n",
    "- Combining customer info with order history\n",
    "- Analyzing game statistics across multiple seasons\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b228071",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Section 2: Data Selection Techniques\n",
    "\n",
    "> \"Selecting the right data is half the battle in data analysis.\"\n",
    "\n",
    "In this section, we'll master the four main selection methods:\n",
    "1. **Column Selection** - Accessing specific columns\n",
    "2. **`.loc[]`** - Label-based selection (inclusive)\n",
    "3. **`.iloc[]`** - Position-based selection (exclusive, like Python lists)\n",
    "4. **Boolean Filtering** - Conditional selection\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¢ Novice Level: Basic Data Selection\n",
    "\n",
    "Let's start with a simple, intuitive dataset to understand the fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Creating Our First DataFrame\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Let's create a simple student dataset to learn selection basics.\n",
    "Think of a DataFrame like an Excel spreadsheet with rows and columns.\n",
    "\"\"\"\n",
    "\n",
    "# Create a simple student DataFrame\n",
    "# Each dictionary key becomes a column name\n",
    "# Each list contains the values for that column\n",
    "student_data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [24, 27, 22, 32, 28],\n",
    "    'Score': [85, 92, 88, 76, 95],\n",
    "    'Grade': ['B', 'A', 'B+', 'C', 'A']\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(student_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"ğŸ“š Our Student DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "print(\"Shape (rows, columns):\", df.shape)\n",
    "print(\"Column names:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Column Selection\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Column selection is the most basic form of data access.\n",
    "Use bracket notation [] with the column name as a string.\n",
    "\"\"\"\n",
    "\n",
    "# Method 1: Select a single column (returns a Series)\n",
    "print(\"1ï¸âƒ£ Select one column (returns a Series):\")\n",
    "print(df['Name'])\n",
    "print()\n",
    "\n",
    "# Method 2: Select multiple columns (returns a DataFrame)\n",
    "# Notice the double brackets [[ ]] - it's a list inside brackets!\n",
    "print(\"2ï¸âƒ£ Select multiple columns (returns a DataFrame):\")\n",
    "print(df[['Name', 'Score']])\n",
    "print()\n",
    "\n",
    "# ğŸ’¡ Key Insight: Single brackets = Series, Double brackets = DataFrame\n",
    "print(\"Type of df['Name']:\", type(df['Name']))\n",
    "print(\"Type of df[['Name']]:\", type(df[['Name']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638391dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: .loc[] vs .iloc[] - The Critical Difference\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "This is ONE OF THE MOST IMPORTANT CONCEPTS in Pandas!\n",
    "\n",
    ".loc[] = LABEL-based selection (think \"Location by Label\")\n",
    "    - Uses row labels (index) and column names\n",
    "    - INCLUDES the endpoint in slices (unlike Python lists!)\n",
    "    \n",
    ".iloc[] = INTEGER POSITION-based selection (think \"Integer Location\")\n",
    "    - Uses integer positions (0, 1, 2, ...)\n",
    "    - EXCLUDES the endpoint (like Python list slicing)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Our DataFrame with index numbers:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# .loc[] Examples - Label-Based\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 50)\n",
    "print(\".loc[] Examples (LABEL-based, INCLUSIVE)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select row with index label 0\n",
    "print(\"\\nğŸ“ df.loc[0] - Get row at index label 0:\")\n",
    "print(df.loc[0])\n",
    "\n",
    "# Select rows 0 through 2 (INCLUDES row 2!)\n",
    "print(\"\\nğŸ“ df.loc[0:2] - Rows 0 to 2 (INCLUSIVE!):\")\n",
    "print(df.loc[0:2])\n",
    "\n",
    "# Select specific rows and columns\n",
    "print(\"\\nğŸ“ df.loc[0:2, ['Name', 'Age']] - Rows 0-2, specific columns:\")\n",
    "print(df.loc[0:2, ['Name', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: .iloc[] - Integer Position Selection\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    ".iloc[] uses integer positions like Python lists!\n",
    "Remember: Python is 0-indexed, and slices EXCLUDE the endpoint.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\".iloc[] Examples (POSITION-based, EXCLUSIVE)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select first row (position 0)\n",
    "print(\"\\nğŸ“ df.iloc[0] - First row (position 0):\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "# Select first 2 rows (positions 0 and 1, NOT 2!)\n",
    "print(\"\\nğŸ“ df.iloc[:2] - First 2 rows (EXCLUDES position 2!):\")\n",
    "print(df.iloc[:2])\n",
    "\n",
    "# Select specific positions\n",
    "print(\"\\nğŸ“ df.iloc[1:4, 0:2] - Rows 1-3, columns 0-1:\")\n",
    "print(df.iloc[1:4, 0:2])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”‘ KEY COMPARISON\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”‘ CRITICAL COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"df.loc[0:2] returns {len(df.loc[0:2])} rows (includes index 2)\")\n",
    "print(f\"df.iloc[:2] returns {len(df.iloc[:2])} rows (excludes position 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f1b0d",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visual: .loc[] vs .iloc[] Comparison\n",
    "\n",
    "Let's create a visual to cement this crucial concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š VISUALIZATION: .loc[] vs .iloc[] Behavior\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "This visualization shows exactly which rows are selected by each method.\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Data for visualization\n",
    "rows = [0, 1, 2, 3, 4]\n",
    "labels = ['Row 0\\n(Alice)', 'Row 1\\n(Bob)', 'Row 2\\n(Charlie)', 'Row 3\\n(David)', 'Row 4\\n(Eve)']\n",
    "\n",
    "# Left plot: .loc[0:2] - includes 3 rows\n",
    "ax1 = axes[0]\n",
    "colors1 = ['#2ecc71', '#2ecc71', '#2ecc71', '#bdc3c7', '#bdc3c7']  # Green for selected\n",
    "bars1 = ax1.barh(rows, [1]*5, color=colors1, edgecolor='black', linewidth=2)\n",
    "ax1.set_yticks(rows)\n",
    "ax1.set_yticklabels(labels)\n",
    "ax1.set_xlim(0, 1.5)\n",
    "ax1.set_title('.loc[0:2] - INCLUSIVE\\n(Selects 3 rows: 0, 1, AND 2)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Selected (Green) vs Not Selected (Gray)')\n",
    "ax1.invert_yaxis()\n",
    "ax1.axhline(y=2.5, color='red', linestyle='--', linewidth=2, label='Endpoint INCLUDED')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "# Right plot: .iloc[:2] - includes only 2 rows\n",
    "ax2 = axes[1]\n",
    "colors2 = ['#3498db', '#3498db', '#bdc3c7', '#bdc3c7', '#bdc3c7']  # Blue for selected\n",
    "bars2 = ax2.barh(rows, [1]*5, color=colors2, edgecolor='black', linewidth=2)\n",
    "ax2.set_yticks(rows)\n",
    "ax2.set_yticklabels(labels)\n",
    "ax2.set_xlim(0, 1.5)\n",
    "ax2.set_title('.iloc[:2] - EXCLUSIVE\\n(Selects 2 rows: 0 and 1, NOT 2)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Selected (Blue) vs Not Selected (Gray)')\n",
    "ax2.invert_yaxis()\n",
    "ax2.axhline(y=1.5, color='red', linestyle='--', linewidth=2, label='Endpoint EXCLUDED')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ”‘ The Critical Difference: loc[] vs iloc[]', fontsize=16, fontweight='bold', y=1.05)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Memory Trick:\")\n",
    "print(\"   â€¢ .loc = Labels, inclusive Like English counting (1, 2, 3)\")\n",
    "print(\"   â€¢ .iloc = Integers, exclusive like Python slicing [start:stop)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99caeee8",
   "metadata": {},
   "source": [
    "### ğŸŸ¢ Novice: Boolean Filtering\n",
    "\n",
    "Boolean filtering lets us select rows based on conditions. Think of it like asking \"Which students scored above 85?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43064dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Boolean Filtering Basics\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Boolean filtering works in two steps:\n",
    "1. Create a boolean mask (True/False for each row)\n",
    "2. Apply the mask to filter the DataFrame\n",
    "\"\"\"\n",
    "\n",
    "print(\"Our DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Step 1: Create a boolean mask\n",
    "# This asks \"Is each student's age greater than 24?\"\n",
    "age_mask = df['Age'] > 24\n",
    "\n",
    "print(\"Step 1 - Boolean mask (df['Age'] > 24):\")\n",
    "print(age_mask)\n",
    "print()\n",
    "\n",
    "# Step 2: Apply the mask to filter\n",
    "# Only rows where the mask is True are returned\n",
    "filtered_df = df[age_mask]\n",
    "\n",
    "print(\"Step 2 - Apply mask (df[age_mask]):\")\n",
    "print(filtered_df)\n",
    "print()\n",
    "\n",
    "# You can combine both steps into one line:\n",
    "print(\"One-liner equivalent: df[df['Age'] > 24]\")\n",
    "print(df[df['Age'] > 24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c8f91",
   "metadata": {},
   "source": [
    "### ğŸŸ¡ Intermediate: Multiple Conditions & String Methods\n",
    "\n",
    "Now let's combine conditions and work with text data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136793e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: Multiple Conditions\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "CRITICAL: When combining conditions in Pandas:\n",
    "    - Use & for AND (not 'and')\n",
    "    - Use | for OR (not 'or')\n",
    "    - Use ~ for NOT (not 'not')\n",
    "    - WRAP EACH CONDITION IN PARENTHESES!\n",
    "\n",
    "This is different from regular Python because of operator precedence.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Multiple Conditions Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# AND condition: Age > 24 AND Score >= 88\n",
    "print(\"\\n1ï¸âƒ£ AND condition: (Age > 24) AND (Score >= 88)\")\n",
    "result_and = df[(df['Age'] > 24) & (df['Score'] >= 88)]\n",
    "print(result_and)\n",
    "\n",
    "# OR condition: Age > 30 OR Score >= 90\n",
    "print(\"\\n2ï¸âƒ£ OR condition: (Age > 30) OR (Score >= 90)\")\n",
    "result_or = df[(df['Age'] > 30) | (df['Score'] >= 90)]\n",
    "print(result_or)\n",
    "\n",
    "# NOT condition: NOT (Age > 24)\n",
    "print(\"\\n3ï¸âƒ£ NOT condition: NOT (Age > 24)\")\n",
    "result_not = df[~(df['Age'] > 24)]\n",
    "print(result_not)\n",
    "\n",
    "# âš ï¸ COMMON MISTAKE DEMONSTRATION\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âš ï¸ COMMON MISTAKE: Forgetting parentheses\")\n",
    "print(\"=\" * 50)\n",
    "print(\"WRONG: df[df['Age'] > 24 & df['Score'] >= 88]\")\n",
    "print(\"RIGHT: df[(df['Age'] > 24) & (df['Score'] >= 88)]\")\n",
    "print(\"\\nWithout parentheses, Python evaluates & before > due to operator precedence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff91194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: String Methods with .str accessor\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "String operations on DataFrame columns require the .str accessor.\n",
    "This gives you access to all string methods like .contains(), .startswith(), etc.\n",
    "\n",
    "CRITICAL: Forgetting .str. is one of the most common Pandas errors!\n",
    "\"\"\"\n",
    "\n",
    "print(\"String Operations Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create DataFrame with more string data\n",
    "names_df = pd.DataFrame({\n",
    "    'Name': ['Alice Smith', 'Bob Johnson', 'Charlie Brown', 'David Lee', 'Eve Wilson'],\n",
    "    'Email': ['alice@gmail.com', 'bob@yahoo.com', 'charlie@gmail.com', 'david@company.com', 'eve@gmail.com'],\n",
    "    'Department': ['Sales', 'IT', 'Sales', 'HR', 'IT']\n",
    "})\n",
    "\n",
    "print(\"Names DataFrame:\")\n",
    "print(names_df)\n",
    "\n",
    "# .str.contains() - Find names containing a substring\n",
    "print(\"\\n1ï¸âƒ£ Names containing 'a' (case-insensitive):\")\n",
    "print(names_df[names_df['Name'].str.contains('a', case=False)])\n",
    "\n",
    "# .str.startswith() - Filter by prefix\n",
    "print(\"\\n2ï¸âƒ£ Emails starting with 'a' or 'e':\")\n",
    "print(names_df[names_df['Email'].str.startswith(('a', 'e'))])\n",
    "\n",
    "# .str.endswith() - Filter by suffix\n",
    "print(\"\\n3ï¸âƒ£ Gmail users:\")\n",
    "print(names_df[names_df['Email'].str.endswith('gmail.com')])\n",
    "\n",
    "# âš ï¸ COMMON MISTAKE\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âš ï¸ COMMON MISTAKE: Forgetting .str.\")\n",
    "print(\"=\" * 50)\n",
    "print(\"WRONG: df['Name'].contains('a')\")\n",
    "print(\"RIGHT: df['Name'].str.contains('a')\")\n",
    "print(\"\\nWithout .str., Python looks for .contains() on the Series object, not strings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc765bc",
   "metadata": {},
   "source": [
    "### ğŸ”´ Advanced: Complex Selection Patterns\n",
    "\n",
    "Now let's see production-ready selection techniques used by data professionals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”´ ADVANCED: Production-Ready Selection Patterns\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "These are patterns you'll see in professional data analysis:\n",
    "1. .query() method - SQL-like syntax for filtering\n",
    "2. .isin() method - Check membership in a list\n",
    "3. Method chaining - Combine operations fluently\n",
    "4. Conditional selection with .where() and .mask()\n",
    "\"\"\"\n",
    "\n",
    "# Create a more realistic dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "sales_df = pd.DataFrame({\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Watch'], 100),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'Sales': np.random.randint(100, 1000, 100),\n",
    "    'Quarter': np.random.choice(['Q1', 'Q2', 'Q3', 'Q4'], 100),\n",
    "    'Year': np.random.choice([2022, 2023, 2024], 100)\n",
    "})\n",
    "\n",
    "print(\"Sales DataFrame (first 10 rows):\")\n",
    "print(sales_df.head(10))\n",
    "print(f\"\\nShape: {sales_df.shape}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 1: .query() - SQL-like filtering\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1ï¸âƒ£ .query() Method - SQL-like Syntax\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Much cleaner than boolean masks for complex conditions!\n",
    "high_laptop_sales = sales_df.query(\"Product == 'Laptop' and Sales > 500 and Region in ['North', 'East']\")\n",
    "print(\"\\nLaptops with Sales > 500 in North or East regions:\")\n",
    "print(high_laptop_sales.head())\n",
    "print(f\"Count: {len(high_laptop_sales)}\")\n",
    "\n",
    "# Using variables in query with @\n",
    "min_sales = 600\n",
    "filtered = sales_df.query(\"Sales >= @min_sales\")\n",
    "print(f\"\\nğŸ“Š Sales >= {min_sales}: {len(filtered)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfe89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”´ ADVANCED: More Selection Techniques\n",
    "# =============================================================================\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 2: .isin() - Check membership in a list\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 50)\n",
    "print(\"2ï¸âƒ£ .isin() Method - Membership Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select only specific products\n",
    "target_products = ['Laptop', 'Tablet']\n",
    "product_selection = sales_df[sales_df['Product'].isin(target_products)]\n",
    "print(f\"\\nRecords for {target_products}:\")\n",
    "print(product_selection.head())\n",
    "print(f\"Count: {len(product_selection)}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 3: Method Chaining - Fluent, readable code\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3ï¸âƒ£ Method Chaining - Professional Style\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Chain multiple operations together\n",
    "result = (\n",
    "    sales_df\n",
    "    .query(\"Year == 2024\")                    # Filter to 2024\n",
    "    .loc[:, ['Product', 'Region', 'Sales']]   # Select columns\n",
    "    .sort_values('Sales', ascending=False)     # Sort by sales\n",
    "    .head(10)                                  # Top 10\n",
    ")\n",
    "print(\"\\nTop 10 sales in 2024 (using method chaining):\")\n",
    "print(result)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 4: .nlargest() and .nsmallest() - Quick top/bottom N\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4ï¸âƒ£ .nlargest() / .nsmallest() - Quick Winners/Losers\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nTop 5 highest sales:\")\n",
    "print(sales_df.nlargest(5, 'Sales'))\n",
    "\n",
    "print(\"\\nBottom 3 lowest sales:\")\n",
    "print(sales_df.nsmallest(3, 'Sales'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec755c",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Check For Understanding: Data Selection\n",
    "\n",
    "**Discussion Questions:**\n",
    "1. What's the difference between `.loc[]` and `.iloc[]`?\n",
    "2. Why do we need parentheses around each condition when using `&`?\n",
    "3. What happens if you forget `.str.` before `.contains()`?\n",
    "4. When would you use `.query()` vs boolean filtering?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ed991",
   "metadata": {},
   "source": [
    "## ğŸ“Š Section 3: Data Aggregation with GroupBy\n",
    "\n",
    "> \"GroupBy is the split-apply-combine pattern â€“ the heart of data analysis.\"\n",
    "\n",
    "The `groupby()` operation is one of the most powerful tools in Pandas. It allows you to:\n",
    "1. **Split** your data into groups based on some criteria\n",
    "2. **Apply** a function to each group independently\n",
    "3. **Combine** the results into a new data structure\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¢ Novice Level: Basic GroupBy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175afd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Understanding GroupBy Conceptually\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "GroupBy is like sorting cards into piles:\n",
    "1. SPLIT: Put all hearts in one pile, all spades in another, etc.\n",
    "2. APPLY: Count cards in each pile\n",
    "3. COMBINE: Report the count for each suit\n",
    "\n",
    "Let's see this with a simple example.\n",
    "\"\"\"\n",
    "\n",
    "# Simple dataset: Products by category\n",
    "simple_data = {\n",
    "    'Category': ['A', 'B', 'A', 'B', 'C', 'A', 'C'],\n",
    "    'Values': [10, 20, 30, 40, 50, 15, 25]\n",
    "}\n",
    "simple_df = pd.DataFrame(simple_data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(simple_df)\n",
    "print()\n",
    "\n",
    "# Step 1: Group by Category (creates a GroupBy object)\n",
    "grouped = simple_df.groupby('Category')\n",
    "print(\"Grouped object:\", type(grouped))\n",
    "print()\n",
    "\n",
    "# Step 2 & 3: Apply sum() and combine results\n",
    "result = grouped.sum()\n",
    "print(\"Sum by Category:\")\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "# One-liner version (most common usage)\n",
    "print(\"One-liner: simple_df.groupby('Category').sum()\")\n",
    "print(simple_df.groupby('Category').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2ebbc",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visual: The Split-Apply-Combine Pattern\n",
    "\n",
    "Let's visualize how GroupBy actually works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3568808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š VISUALIZATION: Split-Apply-Combine Pattern\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "This visualization shows the three stages of groupby operation.\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 5))\n",
    "\n",
    "# Original Data\n",
    "ax1 = axes[0]\n",
    "colors = {'A': '#e74c3c', 'B': '#3498db', 'C': '#2ecc71'}\n",
    "bars = ax1.bar(range(len(simple_df)), simple_df['Values'], \n",
    "               color=[colors[c] for c in simple_df['Category']])\n",
    "ax1.set_xticks(range(len(simple_df)))\n",
    "ax1.set_xticklabels(simple_df['Category'])\n",
    "ax1.set_title('1ï¸âƒ£ Original Data', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Values')\n",
    "\n",
    "# Split Stage\n",
    "ax2 = axes[1]\n",
    "split_text = \"\"\"\n",
    "SPLIT:\n",
    "Group A: [10, 30, 15]\n",
    "Group B: [20, 40]\n",
    "Group C: [50, 25]\n",
    "\"\"\"\n",
    "ax2.text(0.5, 0.5, split_text, ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "         transform=ax2.transAxes, family='monospace')\n",
    "ax2.set_title('2ï¸âƒ£ SPLIT Stage', fontsize=12, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply Stage\n",
    "ax3 = axes[2]\n",
    "apply_text = \"\"\"\n",
    "APPLY (sum):\n",
    "Group A: 10+30+15 = 55\n",
    "Group B: 20+40 = 60\n",
    "Group C: 50+25 = 75\n",
    "\"\"\"\n",
    "ax3.text(0.5, 0.5, apply_text, ha='center', va='center', fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5),\n",
    "         transform=ax3.transAxes, family='monospace')\n",
    "ax3.set_title('3ï¸âƒ£ APPLY Stage', fontsize=12, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Combine Stage\n",
    "ax4 = axes[3]\n",
    "result_grouped = simple_df.groupby('Category')['Values'].sum()\n",
    "ax4.bar(result_grouped.index, result_grouped.values, \n",
    "        color=[colors[c] for c in result_grouped.index])\n",
    "ax4.set_title('4ï¸âƒ£ COMBINE Result', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Sum of Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ“Š GroupBy: Split-Apply-Combine Pattern', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Common Aggregation Functions\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "After grouping, you can apply various aggregation functions:\n",
    "- sum()   : Total of all values\n",
    "- mean()  : Average value\n",
    "- count() : Number of non-null values\n",
    "- min()   : Minimum value\n",
    "- max()   : Maximum value\n",
    "- std()   : Standard deviation\n",
    "- median(): Middle value\n",
    "\"\"\"\n",
    "\n",
    "# Real-world scenario: Regional Sales\n",
    "sales_data = {\n",
    "    'Region': ['North', 'South', 'North', 'South', 'North', 'East', 'East', 'West'],\n",
    "    'Sales': [100, 150, 200, 175, 125, 300, 250, 180],\n",
    "    'Units': [10, 15, 20, 17, 12, 30, 25, 18]\n",
    "}\n",
    "regional_df = pd.DataFrame(sales_data)\n",
    "\n",
    "print(\"Regional Sales Data:\")\n",
    "print(regional_df)\n",
    "print()\n",
    "\n",
    "# Different aggregation functions\n",
    "print(\"=\" * 50)\n",
    "print(\"Common Aggregation Functions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸ“ˆ Sum of Sales by Region:\")\n",
    "print(regional_df.groupby('Region')['Sales'].sum())\n",
    "\n",
    "print(\"\\nğŸ“Š Average Sales by Region:\")\n",
    "print(regional_df.groupby('Region')['Sales'].mean())\n",
    "\n",
    "print(\"\\nğŸ”¢ Count of Transactions by Region:\")\n",
    "print(regional_df.groupby('Region')['Sales'].count())\n",
    "\n",
    "print(\"\\nğŸ“‰ Min and Max Sales by Region:\")\n",
    "print(regional_df.groupby('Region')['Sales'].agg(['min', 'max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122262b",
   "metadata": {},
   "source": [
    "### ğŸŸ¡ Intermediate: Multiple Aggregations with .agg()\n",
    "\n",
    "The `.agg()` method lets you apply multiple aggregation functions at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532abd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: Multiple Aggregations with .agg()\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "The .agg() method is incredibly flexible:\n",
    "1. Apply multiple functions to one column\n",
    "2. Apply different functions to different columns\n",
    "3. Name your output columns\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸŸ¡ INTERMEDIATE: .agg() Method Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Multiple functions on one column (list of functions)\n",
    "print(\"\\n1ï¸âƒ£ Multiple functions on one column:\")\n",
    "result1 = regional_df.groupby('Region')['Sales'].agg(['sum', 'mean', 'count'])\n",
    "print(result1)\n",
    "\n",
    "# Method 2: Different functions for different columns (dictionary)\n",
    "print(\"\\n2ï¸âƒ£ Different functions for different columns:\")\n",
    "result2 = regional_df.groupby('Region').agg({\n",
    "    'Sales': ['sum', 'mean'],      # Sum and mean for Sales\n",
    "    'Units': ['sum', 'count']      # Sum and count for Units\n",
    "})\n",
    "print(result2)\n",
    "\n",
    "# Method 3: Named aggregations (most readable!)\n",
    "print(\"\\n3ï¸âƒ£ Named aggregations (cleanest output):\")\n",
    "result3 = regional_df.groupby('Region').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    avg_sales=('Sales', 'mean'),\n",
    "    total_units=('Units', 'sum'),\n",
    "    num_transactions=('Units', 'count')\n",
    ")\n",
    "print(result3)\n",
    "\n",
    "print(\"\\nğŸ’¡ Named aggregations are best for production code!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: Visualizing Aggregated Data\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Aggregated data is perfect for visualization!\n",
    "Let's create meaningful charts from our grouped data.\n",
    "\"\"\"\n",
    "\n",
    "# Create a more substantial dataset for visualization\n",
    "np.random.seed(42)\n",
    "viz_data = pd.DataFrame({\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 200),\n",
    "    'Product': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], 200),\n",
    "    'Sales': np.random.randint(50, 500, 200),\n",
    "    'Quantity': np.random.randint(1, 20, 200)\n",
    "})\n",
    "\n",
    "# Aggregate by Region\n",
    "regional_summary = viz_data.groupby('Region').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    avg_sales=('Sales', 'mean'),\n",
    "    num_transactions=('Sales', 'count')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Regional Summary:\")\n",
    "print(regional_summary)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Bar chart: Total Sales by Region\n",
    "ax1 = axes[0, 0]\n",
    "colors = sns.color_palette(\"husl\", 4)\n",
    "bars = ax1.bar(regional_summary['Region'], regional_summary['total_sales'], color=colors)\n",
    "ax1.set_title('Total Sales by Region', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Region')\n",
    "ax1.set_ylabel('Total Sales ($)')\n",
    "ax1.bar_label(bars, fmt='$%.0f')\n",
    "\n",
    "# 2. Seaborn: Average Sales by Region (with error bars)\n",
    "ax2 = axes[0, 1]\n",
    "sns.barplot(data=viz_data, x='Region', y='Sales', estimator='mean', \n",
    "            errorbar='sd', palette='husl', ax=ax2)\n",
    "ax2.set_title('Average Sales by Region (with Std Dev)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Average Sales ($)')\n",
    "\n",
    "# 3. Pie chart: Transaction Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.pie(regional_summary['num_transactions'], labels=regional_summary['Region'],\n",
    "        autopct='%1.1f%%', colors=colors, explode=[0.05]*4)\n",
    "ax3.set_title('Transaction Distribution by Region', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Grouped bar: Sales by Region and Product\n",
    "ax4 = axes[1, 1]\n",
    "product_region = viz_data.groupby(['Region', 'Product'])['Sales'].sum().unstack()\n",
    "product_region.plot(kind='bar', ax=ax4, colormap='husl')\n",
    "ax4.set_title('Sales by Region and Product', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Region')\n",
    "ax4.set_ylabel('Total Sales ($)')\n",
    "ax4.legend(title='Product', bbox_to_anchor=(1.02, 1))\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2db810",
   "metadata": {},
   "source": [
    "### ğŸ”´ Advanced: Complex Aggregation Patterns\n",
    "\n",
    "Now let's explore production-level aggregation techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”´ ADVANCED: Multi-Level GroupBy and Transform\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Advanced groupby patterns for complex analysis:\n",
    "1. Multi-level grouping (group by multiple columns)\n",
    "2. Transform (apply function but keep original shape)\n",
    "3. Custom aggregation functions\n",
    "4. Groupby with filtering\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: Multi-Level GroupBy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by multiple columns\n",
    "multi_group = viz_data.groupby(['Region', 'Product']).agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    avg_sales=('Sales', 'mean'),\n",
    "    transactions=('Sales', 'count')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nGrouped by Region AND Product:\")\n",
    "print(multi_group.head(10))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Transform: Apply function but keep original DataFrame shape\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: Transform (Keep Original Shape)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Add regional average as a new column\n",
    "viz_data_copy = viz_data.copy()\n",
    "viz_data_copy['regional_avg'] = viz_data_copy.groupby('Region')['Sales'].transform('mean')\n",
    "viz_data_copy['vs_regional_avg'] = viz_data_copy['Sales'] - viz_data_copy['regional_avg']\n",
    "\n",
    "print(\"\\nOriginal data with regional context:\")\n",
    "print(viz_data_copy[['Region', 'Product', 'Sales', 'regional_avg', 'vs_regional_avg']].head(10))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Custom Aggregation Function\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: Custom Aggregation Functions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def coefficient_of_variation(x):\n",
    "    \"\"\"Calculate coefficient of variation (CV = std/mean * 100).\"\"\"\n",
    "    return (x.std() / x.mean() * 100) if x.mean() != 0 else 0\n",
    "\n",
    "custom_agg = viz_data.groupby('Region').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    sales_cv=('Sales', coefficient_of_variation),  # Custom function!\n",
    "    sales_range=('Sales', lambda x: x.max() - x.min())  # Lambda function!\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nWith custom aggregation functions:\")\n",
    "print(custom_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”´ ADVANCED: GroupBy Visualization with Seaborn\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Seaborn excels at visualizing grouped data with minimal code.\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Box plot: Distribution of Sales by Region\n",
    "ax1 = axes[0, 0]\n",
    "sns.boxplot(data=viz_data, x='Region', y='Sales', palette='husl', ax=ax1)\n",
    "ax1.set_title('Sales Distribution by Region', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Sales ($)')\n",
    "\n",
    "# 2. Violin plot: Distribution with density\n",
    "ax2 = axes[0, 1]\n",
    "sns.violinplot(data=viz_data, x='Region', y='Sales', palette='husl', ax=ax2)\n",
    "ax2.set_title('Sales Density by Region', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Sales ($)')\n",
    "\n",
    "# 3. Point plot: Mean with confidence interval\n",
    "ax3 = axes[1, 0]\n",
    "sns.pointplot(data=viz_data, x='Region', y='Sales', hue='Product', \n",
    "              palette='husl', ax=ax3, errorbar='ci')\n",
    "ax3.set_title('Mean Sales by Region & Product', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Mean Sales ($)')\n",
    "ax3.legend(title='Product', loc='upper right')\n",
    "\n",
    "# 4. Heatmap: Aggregated data\n",
    "ax4 = axes[1, 1]\n",
    "pivot_data = viz_data.groupby(['Region', 'Product'])['Sales'].mean().unstack()\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax4)\n",
    "ax4.set_title('Average Sales Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Seaborn makes complex grouped visualizations simple!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00487316",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Check For Understanding: Data Aggregation\n",
    "\n",
    "**Discussion Questions:**\n",
    "1. What does `groupby()` actually do to the data?\n",
    "2. What's the difference between passing `'sum'` vs `['sum', 'mean']` to `agg()`?\n",
    "3. How would you find the region with the highest total sales?\n",
    "4. When would you use `transform()` instead of `agg()`?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074bae6",
   "metadata": {},
   "source": [
    "## ğŸ”— Section 4: Merging DataFrames\n",
    "\n",
    "> \"Real-world data lives in multiple tables. Merging brings it together.\"\n",
    "\n",
    "Merging is like combining puzzle pieces from different boxes to see the complete picture. There are four main types of joins:\n",
    "\n",
    "| Join Type | Description | Visual |\n",
    "|-----------|-------------|--------|\n",
    "| **Inner** | Only matching rows from both | âˆ© (intersection) |\n",
    "| **Left** | All from left + matches from right | Left circle + overlap |\n",
    "| **Right** | All from right + matches from left | Right circle + overlap |\n",
    "| **Outer** | All rows from both | âˆª (union) |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¢ Novice Level: Understanding Join Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de80518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Creating DataFrames for Merging\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Let's create two related DataFrames to demonstrate merging.\n",
    "Think of these as two separate database tables that need to be combined.\n",
    "\"\"\"\n",
    "\n",
    "# Customers table: basic customer info\n",
    "customers = pd.DataFrame({\n",
    "    'CustomerID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'City': ['New York', 'Boston', 'Chicago', 'Denver']\n",
    "})\n",
    "\n",
    "# Orders table: customer orders (notice not all customers have orders!)\n",
    "orders = pd.DataFrame({\n",
    "    'CustomerID': [1, 2, 5],  # Note: ID 5 is not in customers!\n",
    "    'OrderID': ['O101', 'O102', 'O103'],\n",
    "    'Amount': [250, 150, 300]\n",
    "})\n",
    "\n",
    "print(\"ğŸ‘¥ Customers Table:\")\n",
    "print(customers)\n",
    "print()\n",
    "print(\"ğŸ“¦ Orders Table:\")\n",
    "print(orders)\n",
    "print()\n",
    "print(\"ğŸ” Key Observations:\")\n",
    "print(\"   - CustomerIDs 3 and 4 have NO orders\")\n",
    "print(\"   - CustomerID 5 has an order but is NOT in the customers table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: The Four Types of Joins\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Each join type answers a different business question:\n",
    "- INNER: \"Show me customers who have placed orders\"\n",
    "- LEFT: \"Show me ALL customers, with their orders if any\"\n",
    "- RIGHT: \"Show me ALL orders, with customer info if available\"\n",
    "- OUTER: \"Show me everything from both tables\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”— JOIN TYPE DEMONSTRATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. INNER JOIN: Only rows with matching keys in BOTH tables\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1ï¸âƒ£ INNER JOIN: Only matching rows\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "inner_join = pd.merge(customers, orders, on='CustomerID', how='inner')\n",
    "print(inner_join)\n",
    "print(\"\\nğŸ“Œ Result: Only Alice and Bob (IDs 1 & 2) - they exist in BOTH tables\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. LEFT JOIN: All rows from left table, matching from right\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2ï¸âƒ£ LEFT JOIN: All from customers (left), matching orders\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "left_join = pd.merge(customers, orders, on='CustomerID', how='left')\n",
    "print(left_join)\n",
    "print(\"\\nğŸ“Œ Result: ALL customers shown. Charlie & David have NaN for orders\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. RIGHT JOIN: All rows from right table, matching from left\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3ï¸âƒ£ RIGHT JOIN: All from orders (right), matching customers\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "right_join = pd.merge(customers, orders, on='CustomerID', how='right')\n",
    "print(right_join)\n",
    "print(\"\\nğŸ“Œ Result: ALL orders shown. CustomerID 5 has NaN for Name/City\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. OUTER JOIN: All rows from BOTH tables\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4ï¸âƒ£ OUTER JOIN: Everything from both tables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "outer_join = pd.merge(customers, orders, on='CustomerID', how='outer')\n",
    "print(outer_join)\n",
    "print(\"\\nğŸ“Œ Result: ALL rows from both - NaN where no match exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c07dc4",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visual: Venn Diagrams of Join Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š VISUALIZATION: Venn Diagrams for Join Types\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Visual representation of how each join type selects data.\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "def draw_venn(ax, title, left_color, right_color, overlap_color, highlight='both'):\n",
    "    \"\"\"Helper function to draw Venn diagrams for joins.\"\"\"\n",
    "    # Create circles\n",
    "    left_circle = plt.Circle((-0.3, 0), 0.5, fill=False, linewidth=3, color='black')\n",
    "    right_circle = plt.Circle((0.3, 0), 0.5, fill=False, linewidth=3, color='black')\n",
    "    \n",
    "    # Fill based on join type\n",
    "    if highlight == 'inner':\n",
    "        # Only intersection\n",
    "        wedge = plt.Circle((0, 0), 0.3, color=overlap_color, alpha=0.7)\n",
    "        ax.add_patch(wedge)\n",
    "    elif highlight == 'left':\n",
    "        # Left circle fully filled\n",
    "        ax.add_patch(plt.Circle((-0.3, 0), 0.5, color=left_color, alpha=0.5))\n",
    "        ax.add_patch(plt.Circle((0, 0), 0.2, color=overlap_color, alpha=0.7))\n",
    "    elif highlight == 'right':\n",
    "        # Right circle fully filled\n",
    "        ax.add_patch(plt.Circle((0.3, 0), 0.5, color=right_color, alpha=0.5))\n",
    "        ax.add_patch(plt.Circle((0, 0), 0.2, color=overlap_color, alpha=0.7))\n",
    "    elif highlight == 'outer':\n",
    "        # Both circles fully filled\n",
    "        ax.add_patch(plt.Circle((-0.3, 0), 0.5, color=left_color, alpha=0.5))\n",
    "        ax.add_patch(plt.Circle((0.3, 0), 0.5, color=right_color, alpha=0.5))\n",
    "    \n",
    "    ax.add_patch(left_circle)\n",
    "    ax.add_patch(right_circle)\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(-0.6, 0, 'Customers', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(0.6, 0, 'Orders', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(-0.8, 0.8)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Draw each join type\n",
    "draw_venn(axes[0, 0], 'INNER JOIN\\nOnly matching rows', '#3498db', '#e74c3c', '#9b59b6', 'inner')\n",
    "draw_venn(axes[0, 1], 'LEFT JOIN\\nAll from left + matches', '#3498db', '#e74c3c', '#9b59b6', 'left')\n",
    "draw_venn(axes[1, 0], 'RIGHT JOIN\\nAll from right + matches', '#3498db', '#e74c3c', '#9b59b6', 'right')\n",
    "draw_venn(axes[1, 1], 'OUTER JOIN\\nAll from both', '#3498db', '#e74c3c', '#9b59b6', 'outer')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ”— Visual Guide to Join Types', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Œ Use Case Guide:\")\n",
    "print(\"   INNER: 'Customers who have ordered'\")\n",
    "print(\"   LEFT:  'All customers, with orders if they have any'\")\n",
    "print(\"   RIGHT: 'All orders, with customer info if available'\")\n",
    "print(\"   OUTER: 'Complete picture of all customers and orders'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcd261",
   "metadata": {},
   "source": [
    "### ğŸŸ¡ Intermediate: Handling Column Conflicts and NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fa322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: Column Conflicts with Suffixes\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "When both DataFrames have columns with the same name (besides the join key),\n",
    "Pandas adds suffixes to distinguish them. You can customize these!\n",
    "\"\"\"\n",
    "\n",
    "# Create tables with overlapping column names\n",
    "employees_2022 = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Salary': [50000, 60000, 55000],\n",
    "    'Department': ['Sales', 'IT', 'HR']\n",
    "})\n",
    "\n",
    "employees_2023 = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 4],\n",
    "    'Salary': [52000, 65000, 48000],  # Same column name!\n",
    "    'Department': ['Marketing', 'IT', 'Sales']  # Same column name!\n",
    "})\n",
    "\n",
    "print(\"2022 Employee Data:\")\n",
    "print(employees_2022)\n",
    "print()\n",
    "print(\"2023 Employee Data:\")\n",
    "print(employees_2023)\n",
    "\n",
    "# Merge with default suffixes (_x, _y)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Default Suffixes (_x, _y):\")\n",
    "print(\"=\" * 60)\n",
    "default_merge = pd.merge(employees_2022, employees_2023, on='EmployeeID', how='outer')\n",
    "print(default_merge)\n",
    "\n",
    "# Merge with custom suffixes\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Custom Suffixes (_2022, _2023):\")\n",
    "print(\"=\" * 60)\n",
    "custom_merge = pd.merge(\n",
    "    employees_2022, employees_2023, \n",
    "    on='EmployeeID', \n",
    "    how='outer',\n",
    "    suffixes=('_2022', '_2023')  # Much clearer!\n",
    ")\n",
    "print(custom_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: Handling NaN Values After Merging\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "After outer/left/right joins, you'll often have NaN values.\n",
    "Here's how to handle them properly.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸŸ¡ INTERMEDIATE: Handling NaN Values\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with our merged data\n",
    "merged_df = custom_merge.copy()\n",
    "print(\"\\nOriginal merged data with NaN values:\")\n",
    "print(merged_df)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 1: fillna() - Replace NaN with a specific value\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"1ï¸âƒ£ fillna() - Replace NaN with default value\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "filled_df = merged_df.copy()\n",
    "filled_df['Salary_2023'] = filled_df['Salary_2023'].fillna(0)\n",
    "filled_df['Department_2023'] = filled_df['Department_2023'].fillna('Unknown')\n",
    "print(filled_df)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 2: np.where() - Conditional value selection\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"2ï¸âƒ£ np.where() - Pick value based on condition\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use 2023 salary if available, otherwise use 2022 salary\n",
    "merged_df['Current_Salary'] = np.where(\n",
    "    merged_df['Salary_2023'].notna(),  # Condition\n",
    "    merged_df['Salary_2023'],           # If True\n",
    "    merged_df['Salary_2022']            # If False\n",
    ")\n",
    "print(merged_df[['EmployeeID', 'Name', 'Salary_2022', 'Salary_2023', 'Current_Salary']])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 3: dropna() - Remove rows with NaN\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"3ï¸âƒ£ dropna() - Remove rows with any NaN\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "clean_df = merged_df.dropna()\n",
    "print(clean_df)\n",
    "print(f\"\\nğŸ“Š Rows before: {len(merged_df)}, after: {len(clean_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6dac95",
   "metadata": {},
   "source": [
    "### ğŸ”´ Advanced: Complex Merging Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”´ ADVANCED: Merging on Multiple Keys\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Sometimes you need to match on multiple columns, not just one.\n",
    "This is common with composite keys in databases.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: Multi-Key Merges\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sales data by region and product\n",
    "sales_q1 = pd.DataFrame({\n",
    "    'Region': ['North', 'North', 'South', 'South'],\n",
    "    'Product': ['Laptop', 'Phone', 'Laptop', 'Phone'],\n",
    "    'Q1_Sales': [100, 150, 200, 175]\n",
    "})\n",
    "\n",
    "sales_q2 = pd.DataFrame({\n",
    "    'Region': ['North', 'North', 'South', 'East'],\n",
    "    'Product': ['Laptop', 'Phone', 'Laptop', 'Laptop'],\n",
    "    'Q2_Sales': [120, 180, 190, 250]\n",
    "})\n",
    "\n",
    "print(\"Q1 Sales:\")\n",
    "print(sales_q1)\n",
    "print()\n",
    "print(\"Q2 Sales:\")\n",
    "print(sales_q2)\n",
    "\n",
    "# Merge on BOTH Region AND Product\n",
    "multi_key_merge = pd.merge(\n",
    "    sales_q1, sales_q2,\n",
    "    on=['Region', 'Product'],  # Multiple keys!\n",
    "    how='outer'\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Merged on Region AND Product:\")\n",
    "print(\"=\" * 40)\n",
    "print(multi_key_merge)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Merging with Different Column Names\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: Merging with Different Column Names\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# When key columns have different names in each DataFrame\n",
    "customers_v2 = pd.DataFrame({\n",
    "    'CustID': [1, 2, 3],  # Different name!\n",
    "    'Name': ['Alice', 'Bob', 'Charlie']\n",
    "})\n",
    "\n",
    "orders_v2 = pd.DataFrame({\n",
    "    'CustomerNum': [1, 2, 4],  # Different name!\n",
    "    'Amount': [100, 200, 150]\n",
    "})\n",
    "\n",
    "# Use left_on and right_on\n",
    "merged_diff_keys = pd.merge(\n",
    "    customers_v2, orders_v2,\n",
    "    left_on='CustID',       # Key in left DataFrame\n",
    "    right_on='CustomerNum',  # Key in right DataFrame\n",
    "    how='outer'\n",
    ")\n",
    "print(\"\\nMerged with different key names:\")\n",
    "print(merged_diff_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d7b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”´ ADVANCED: Concatenation with pd.concat()\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "pd.concat() is different from merge():\n",
    "- merge(): Combines columns (horizontal - like SQL JOIN)\n",
    "- concat(): Stacks DataFrames (vertical - like SQL UNION)\n",
    "\n",
    "Use concat when you have the same columns in multiple DataFrames.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: pd.concat() - Stacking DataFrames\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Same structure, different data\n",
    "jan_data = pd.DataFrame({\n",
    "    'Date': ['2024-01-01', '2024-01-15'],\n",
    "    'Sales': [100, 150],\n",
    "    'Region': ['North', 'South']\n",
    "})\n",
    "\n",
    "feb_data = pd.DataFrame({\n",
    "    'Date': ['2024-02-01', '2024-02-15'],\n",
    "    'Sales': [200, 175],\n",
    "    'Region': ['East', 'West']\n",
    "})\n",
    "\n",
    "print(\"January Data:\")\n",
    "print(jan_data)\n",
    "print()\n",
    "print(\"February Data:\")\n",
    "print(feb_data)\n",
    "\n",
    "# Stack vertically (axis=0 is default)\n",
    "combined = pd.concat([jan_data, feb_data], ignore_index=True)\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Combined with pd.concat():\")\n",
    "print(\"=\" * 40)\n",
    "print(combined)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Visualize the difference between merge and concat\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Concat visualization (vertical stacking)\n",
    "ax1 = axes[0]\n",
    "ax1.text(0.5, 0.8, 'DataFrame A', ha='center', fontsize=14, \n",
    "         bbox=dict(boxstyle='round', facecolor='#3498db', alpha=0.7))\n",
    "ax1.text(0.5, 0.5, 'â†“ pd.concat() â†“', ha='center', fontsize=12, fontweight='bold')\n",
    "ax1.text(0.5, 0.2, 'DataFrame B', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.7))\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title('pd.concat()\\n(Stack Vertically)', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Merge visualization (horizontal joining)\n",
    "ax2 = axes[1]\n",
    "ax2.text(0.25, 0.5, 'DataFrame A', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round', facecolor='#3498db', alpha=0.7))\n",
    "ax2.text(0.5, 0.5, 'â† pd.merge() â†’', ha='center', fontsize=12, fontweight='bold')\n",
    "ax2.text(0.75, 0.5, 'DataFrame B', ha='center', fontsize=14,\n",
    "         bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.7))\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('pd.merge()\\n(Join Horizontally)', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fde95",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Check For Understanding: Merging\n",
    "\n",
    "**Discussion Questions:**\n",
    "1. When would you use an inner join vs an outer join?\n",
    "2. What does `NaN` mean and when does it appear after merging?\n",
    "3. If both DataFrames have an 'Age' column, what happens?\n",
    "4. Which join type would you use to keep all customers even if they haven't ordered?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a381110",
   "metadata": {},
   "source": [
    "## ğŸ”„ Section 5: Data Transformation\n",
    "\n",
    "> \"Transforming data is where analysis becomes insight.\"\n",
    "\n",
    "Data transformation involves:\n",
    "1. **Creating new columns** from existing ones\n",
    "2. **Modifying values** with operators and functions\n",
    "3. **Applying custom logic** with `map()` and `apply()`\n",
    "4. **Renaming and dropping** columns\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŸ¢ Novice Level: Basic Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Creating and Modifying Columns\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Column transformation is one of the most common operations in data analysis.\n",
    "You can create new columns or modify existing ones using simple operators.\n",
    "\"\"\"\n",
    "\n",
    "# Create a sample employee dataset\n",
    "employees = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Department': ['Sales', 'IT', 'Sales', 'HR', 'IT'],\n",
    "    'Salary': [50000, 60000, 55000, 45000, 70000],\n",
    "    'Years': [3, 5, 2, 7, 4]\n",
    "})\n",
    "\n",
    "print(\"Original Employee Data:\")\n",
    "print(employees)\n",
    "print()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Creating New Columns\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"1ï¸âƒ£ Creating New Columns with Operators\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple calculation: 10% bonus\n",
    "employees['Bonus'] = employees['Salary'] * 0.10\n",
    "print(\"\\nAdded 10% Bonus column:\")\n",
    "print(employees[['Name', 'Salary', 'Bonus']])\n",
    "\n",
    "# Combining columns: Total compensation\n",
    "employees['Total_Comp'] = employees['Salary'] + employees['Bonus']\n",
    "print(\"\\nAdded Total Compensation:\")\n",
    "print(employees[['Name', 'Salary', 'Bonus', 'Total_Comp']])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Modifying Existing Columns\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2ï¸âƒ£ Modifying Existing Columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Give everyone a 5% raise\n",
    "employees['Salary'] = employees['Salary'] * 1.05\n",
    "print(\"\\nAfter 5% raise:\")\n",
    "print(employees[['Name', 'Salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¢ NOVICE: Renaming and Dropping Columns\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Renaming columns improves readability.\n",
    "Dropping columns removes unnecessary data.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"3ï¸âƒ£ Renaming Columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Rename a single column\n",
    "employees_renamed = employees.rename(columns={'Total_Comp': 'Total_Compensation'})\n",
    "print(\"\\nRenamed 'Total_Comp' to 'Total_Compensation':\")\n",
    "print(employees_renamed.columns.tolist())\n",
    "\n",
    "# Rename multiple columns at once\n",
    "employees_renamed = employees.rename(columns={\n",
    "    'Years': 'Experience_Years',\n",
    "    'Bonus': 'Annual_Bonus'\n",
    "})\n",
    "print(\"\\nRenamed multiple columns:\")\n",
    "print(employees_renamed.columns.tolist())\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Dropping Columns\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4ï¸âƒ£ Dropping Columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Drop a single column\n",
    "# axis=1 means columns (axis=0 would mean rows)\n",
    "employees_slim = employees.drop('Bonus', axis=1)\n",
    "print(\"\\nDropped 'Bonus' column:\")\n",
    "print(employees_slim.columns.tolist())\n",
    "\n",
    "# Drop multiple columns\n",
    "employees_minimal = employees.drop(['Bonus', 'Total_Comp'], axis=1)\n",
    "print(\"\\nDropped 'Bonus' and 'Total_Comp':\")\n",
    "print(employees_minimal)\n",
    "\n",
    "print(\"\\nğŸ’¡ Note: drop() returns a new DataFrame. Use inplace=True to modify in place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef4598",
   "metadata": {},
   "source": [
    "### ğŸŸ¡ Intermediate: Using map() and apply() for Custom Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9598410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: map() for Element-wise Transformations\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "map() applies a function to each element in a Series.\n",
    "Perfect for:\n",
    "- Category encoding\n",
    "- Value mapping with dictionaries\n",
    "- Simple conditional logic\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸŸ¡ INTERMEDIATE: Using map()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fresh employee data\n",
    "emp_df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Department': ['Sales', 'IT', 'Sales', 'HR', 'IT'],\n",
    "    'Salary': [50000, 60000, 55000, 45000, 70000],\n",
    "    'Performance': ['Excellent', 'Good', 'Average', 'Excellent', 'Good']\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(emp_df)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 1: map() with a dictionary (value mapping)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"1ï¸âƒ£ map() with Dictionary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create performance score mapping\n",
    "performance_scores = {\n",
    "    'Excellent': 5,\n",
    "    'Good': 4,\n",
    "    'Average': 3,\n",
    "    'Poor': 2\n",
    "}\n",
    "\n",
    "emp_df['Perf_Score'] = emp_df['Performance'].map(performance_scores)\n",
    "print(\"\\nAdded Performance Score:\")\n",
    "print(emp_df[['Name', 'Performance', 'Perf_Score']])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 2: map() with a lambda function\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"2ï¸âƒ£ map() with Lambda Function\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Categorize salary levels\n",
    "emp_df['Salary_Level'] = emp_df['Salary'].map(\n",
    "    lambda x: 'High' if x >= 60000 else ('Medium' if x >= 50000 else 'Low')\n",
    ")\n",
    "print(\"\\nAdded Salary Level:\")\n",
    "print(emp_df[['Name', 'Salary', 'Salary_Level']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814dda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸŸ¡ INTERMEDIATE: apply() for Row-wise and Column-wise Operations\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "apply() is more flexible than map():\n",
    "- Can operate on entire rows (axis=1)\n",
    "- Can operate on entire columns (axis=0)\n",
    "- Works with DataFrames and Series\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸŸ¡ INTERMEDIATE: Using apply()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 1: apply() on a Series (like map)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"1ï¸âƒ£ apply() on a Series\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def categorize_salary(salary):\n",
    "    \"\"\"Categorize salary into tax brackets.\"\"\"\n",
    "    if salary >= 70000:\n",
    "        return 'High Bracket'\n",
    "    elif salary >= 50000:\n",
    "        return 'Medium Bracket'\n",
    "    else:\n",
    "        return 'Low Bracket'\n",
    "\n",
    "emp_df['Tax_Bracket'] = emp_df['Salary'].apply(categorize_salary)\n",
    "print(\"\\nAdded Tax Bracket:\")\n",
    "print(emp_df[['Name', 'Salary', 'Tax_Bracket']])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Method 2: apply() across rows (axis=1)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"2ï¸âƒ£ apply() Across Rows (axis=1)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def calculate_bonus(row):\n",
    "    \"\"\"Calculate bonus based on department and performance.\"\"\"\n",
    "    base_bonus = row['Salary'] * 0.1\n",
    "    \n",
    "    # IT gets extra 5%\n",
    "    if row['Department'] == 'IT':\n",
    "        base_bonus *= 1.05\n",
    "    \n",
    "    # Excellent performance gets 20% extra\n",
    "    if row['Performance'] == 'Excellent':\n",
    "        base_bonus *= 1.2\n",
    "    \n",
    "    return round(base_bonus, 2)\n",
    "\n",
    "emp_df['Calculated_Bonus'] = emp_df.apply(calculate_bonus, axis=1)\n",
    "print(\"\\nBonus based on Department AND Performance:\")\n",
    "print(emp_df[['Name', 'Department', 'Performance', 'Salary', 'Calculated_Bonus']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b66e75",
   "metadata": {},
   "source": [
    "### ğŸ”´ Advanced: Vectorized Operations and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a47d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”´ ADVANCED: Vectorized Operations (Best Practice)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Vectorized operations are MUCH faster than apply() or loops.\n",
    "Always prefer vectorized operations when possible!\n",
    "\n",
    "Performance hierarchy (fastest to slowest):\n",
    "1. Vectorized NumPy/Pandas operations\n",
    "2. .str./ accessor methods\n",
    "3. .apply() with NumPy\n",
    "4. .apply() with Python functions\n",
    "5. Python loops (AVOID!)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: Vectorized Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create larger dataset for performance comparison\n",
    "np.random.seed(42)\n",
    "large_df = pd.DataFrame({\n",
    "    'A': np.random.randint(1, 100, 10000),\n",
    "    'B': np.random.randint(1, 100, 10000),\n",
    "    'Category': np.random.choice(['X', 'Y', 'Z'], 10000)\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {len(large_df):,} rows\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Vectorized vs Non-Vectorized Comparison\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import time\n",
    "\n",
    "# Method 1: Vectorized (FAST)\n",
    "start = time.time()\n",
    "large_df['C_vectorized'] = large_df['A'] + large_df['B'] * 2\n",
    "vectorized_time = time.time() - start\n",
    "\n",
    "# Method 2: Apply (SLOW)\n",
    "start = time.time()\n",
    "large_df['C_apply'] = large_df.apply(lambda row: row['A'] + row['B'] * 2, axis=1)\n",
    "apply_time = time.time() - start\n",
    "\n",
    "print(f\"\\nâš¡ Vectorized: {vectorized_time*1000:.2f} ms\")\n",
    "print(f\"ğŸ¢ Apply:      {apply_time*1000:.2f} ms\")\n",
    "print(f\"ğŸ“Š Speedup:    {apply_time/vectorized_time:.1f}x faster!\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Vectorized conditional with np.where() and np.select()\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”´ ADVANCED: Vectorized Conditionals\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# np.where() - Simple if/else\n",
    "large_df['Size'] = np.where(large_df['A'] > 50, 'Large', 'Small')\n",
    "print(\"\\nnp.where() - Simple if/else:\")\n",
    "print(large_df[['A', 'Size']].head(10))\n",
    "\n",
    "# np.select() - Multiple conditions\n",
    "conditions = [\n",
    "    (large_df['A'] <= 33),\n",
    "    (large_df['A'] <= 66),\n",
    "    (large_df['A'] > 66)\n",
    "]\n",
    "choices = ['Low', 'Medium', 'High']\n",
    "large_df['Level'] = np.select(conditions, choices, default='Unknown')\n",
    "\n",
    "print(\"\\nnp.select() - Multiple conditions:\")\n",
    "print(large_df[['A', 'Level']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f65630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š VISUALIZATION: Transformation Results\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Let's visualize the transformations we've made to our employee data.\n",
    "\"\"\"\n",
    "\n",
    "# Recreate a clean employee dataset with transformations\n",
    "viz_emp = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'Department': ['Sales', 'IT', 'Sales', 'HR', 'IT', 'Sales', 'HR', 'IT'],\n",
    "    'Base_Salary': [50000, 60000, 55000, 45000, 70000, 48000, 52000, 65000],\n",
    "    'Performance': ['Excellent', 'Good', 'Average', 'Excellent', 'Good', 'Average', 'Good', 'Excellent']\n",
    "})\n",
    "\n",
    "# Add calculated columns\n",
    "viz_emp['Bonus'] = viz_emp['Base_Salary'] * np.where(viz_emp['Performance'] == 'Excellent', 0.15,\n",
    "                                                     np.where(viz_emp['Performance'] == 'Good', 0.10, 0.05))\n",
    "viz_emp['Total_Comp'] = viz_emp['Base_Salary'] + viz_emp['Bonus']\n",
    "\n",
    "print(\"Employee Data with Transformations:\")\n",
    "print(viz_emp)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Salary Distribution by Department\n",
    "ax1 = axes[0, 0]\n",
    "dept_salary = viz_emp.groupby('Department')['Base_Salary'].mean()\n",
    "colors = sns.color_palette(\"husl\", len(dept_salary))\n",
    "bars = ax1.bar(dept_salary.index, dept_salary.values, color=colors)\n",
    "ax1.set_title('Average Salary by Department', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Average Salary ($)')\n",
    "ax1.bar_label(bars, fmt='$%.0f')\n",
    "\n",
    "# 2. Compensation Breakdown (Stacked Bar)\n",
    "ax2 = axes[0, 1]\n",
    "x = np.arange(len(viz_emp))\n",
    "width = 0.6\n",
    "ax2.bar(x, viz_emp['Base_Salary'], width, label='Base Salary', color='#3498db')\n",
    "ax2.bar(x, viz_emp['Bonus'], width, bottom=viz_emp['Base_Salary'], label='Bonus', color='#e74c3c')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(viz_emp['Name'], rotation=45, ha='right')\n",
    "ax2.set_title('Total Compensation Breakdown', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Compensation ($)')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Performance Distribution\n",
    "ax3 = axes[1, 0]\n",
    "perf_counts = viz_emp['Performance'].value_counts()\n",
    "ax3.pie(perf_counts.values, labels=perf_counts.index, autopct='%1.1f%%',\n",
    "        colors=sns.color_palette(\"husl\", len(perf_counts)), explode=[0.05]*len(perf_counts))\n",
    "ax3.set_title('Performance Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Bonus vs Base Salary Scatter\n",
    "ax4 = axes[1, 1]\n",
    "colors_perf = {'Excellent': '#2ecc71', 'Good': '#f39c12', 'Average': '#e74c3c'}\n",
    "for perf in viz_emp['Performance'].unique():\n",
    "    mask = viz_emp['Performance'] == perf\n",
    "    ax4.scatter(viz_emp.loc[mask, 'Base_Salary'], viz_emp.loc[mask, 'Bonus'],\n",
    "                label=perf, s=100, c=colors_perf[perf], alpha=0.7)\n",
    "ax4.set_xlabel('Base Salary ($)')\n",
    "ax4.set_ylabel('Bonus ($)')\n",
    "ax4.set_title('Bonus vs Base Salary by Performance', fontsize=14, fontweight='bold')\n",
    "ax4.legend(title='Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679acfa",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Check For Understanding: Data Transformation\n",
    "\n",
    "**Discussion Questions:**\n",
    "1. Why can't we write `df['Name'] = df['Name'].upper()`?\n",
    "2. When would you use `map()` vs just an operator like `*`?\n",
    "3. What does `axis=1` mean in `apply()` and `drop()`?\n",
    "4. Why are vectorized operations faster than `apply()`?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f8c21",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ Section 6: Comprehensive Practice\n",
    "\n",
    "> \"Practice makes permanent. Let's put it all together!\"\n",
    "\n",
    "Now let's work through a real-world scenario that combines **selection**, **aggregation**, **merging**, and **transformation**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Scenario: Sales Analysis\n",
    "\n",
    "Imagine you're a data analyst at a retail company. You have sales data from multiple sources that need to be combined and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94400838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ‹ï¸ COMPREHENSIVE PRACTICE: Creating the Datasets\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "We'll work with three related datasets:\n",
    "1. Products: Information about products\n",
    "2. Sales: Daily sales transactions\n",
    "3. Stores: Store location information\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset 1: Products\n",
    "products = pd.DataFrame({\n",
    "    'ProductID': range(1, 11),\n",
    "    'ProductName': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones',\n",
    "                    'Keyboard', 'Mouse', 'Monitor', 'Speaker', 'Charger'],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Wearables', 'Audio',\n",
    "                 'Accessories', 'Accessories', 'Electronics', 'Audio', 'Accessories'],\n",
    "    'UnitPrice': [999.99, 699.99, 449.99, 299.99, 149.99,\n",
    "                  79.99, 29.99, 349.99, 99.99, 19.99]\n",
    "})\n",
    "\n",
    "# Dataset 2: Sales Transactions\n",
    "n_sales = 500\n",
    "sales = pd.DataFrame({\n",
    "    'TransactionID': range(1, n_sales + 1),\n",
    "    'Date': pd.date_range('2024-01-01', periods=n_sales, freq='4H'),\n",
    "    'StoreID': np.random.choice(range(1, 6), n_sales),\n",
    "    'ProductID': np.random.choice(range(1, 11), n_sales),\n",
    "    'Quantity': np.random.randint(1, 10, n_sales),\n",
    "    'Discount': np.random.choice([0, 0.05, 0.10, 0.15, 0.20], n_sales)\n",
    "})\n",
    "\n",
    "# Dataset 3: Stores\n",
    "stores = pd.DataFrame({\n",
    "    'StoreID': range(1, 6),\n",
    "    'StoreName': ['Downtown', 'Mall Central', 'Airport', 'University', 'Suburb'],\n",
    "    'Region': ['North', 'Central', 'East', 'Central', 'West'],\n",
    "    'ManagerName': ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n",
    "})\n",
    "\n",
    "print(\"ğŸ“¦ Products Table:\")\n",
    "print(products)\n",
    "print()\n",
    "print(\"ğŸ›’ Sales Table (first 10 rows):\")\n",
    "print(sales.head(10))\n",
    "print(f\"   ... ({len(sales)} total transactions)\")\n",
    "print()\n",
    "print(\"ğŸª Stores Table:\")\n",
    "print(stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ‹ï¸ STEP 1: Merging Datasets Together\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Let's combine all three datasets to get a complete picture of each transaction.\n",
    "\n",
    "Step by step:\n",
    "1. Merge sales with products (to get product info)\n",
    "2. Merge result with stores (to get store info)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”— STEP 1: MERGING DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First merge: Sales + Products\n",
    "sales_with_products = pd.merge(\n",
    "    sales, \n",
    "    products, \n",
    "    on='ProductID', \n",
    "    how='left'\n",
    ")\n",
    "print(\"\\n1ï¸âƒ£ After merging Sales with Products:\")\n",
    "print(f\"   Shape: {sales_with_products.shape}\")\n",
    "print(f\"   Columns: {list(sales_with_products.columns)}\")\n",
    "\n",
    "# Second merge: Add store information\n",
    "full_data = pd.merge(\n",
    "    sales_with_products, \n",
    "    stores, \n",
    "    on='StoreID', \n",
    "    how='left'\n",
    ")\n",
    "print(\"\\n2ï¸âƒ£ After merging with Stores:\")\n",
    "print(f\"   Shape: {full_data.shape}\")\n",
    "print(f\"   Columns: {list(full_data.columns)}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Complete merged dataset (first 5 rows):\")\n",
    "print(full_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ‹ï¸ STEP 2: Transforming Data (Creating Calculated Columns)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Now let's calculate important metrics:\n",
    "1. Revenue before discount\n",
    "2. Discount amount\n",
    "3. Final revenue after discount\n",
    "4. Extract date components\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”„ STEP 2: DATA TRANSFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate revenue metrics\n",
    "full_data['Gross_Revenue'] = full_data['UnitPrice'] * full_data['Quantity']\n",
    "full_data['Discount_Amount'] = full_data['Gross_Revenue'] * full_data['Discount']\n",
    "full_data['Net_Revenue'] = full_data['Gross_Revenue'] - full_data['Discount_Amount']\n",
    "\n",
    "# Extract date components for time-based analysis\n",
    "full_data['Month'] = full_data['Date'].dt.month\n",
    "full_data['DayOfWeek'] = full_data['Date'].dt.day_name()\n",
    "full_data['Week'] = full_data['Date'].dt.isocalendar().week\n",
    "\n",
    "print(\"\\nğŸ“Š With calculated columns:\")\n",
    "print(full_data[['ProductName', 'Quantity', 'UnitPrice', 'Discount', \n",
    "                  'Gross_Revenue', 'Discount_Amount', 'Net_Revenue']].head(10))\n",
    "\n",
    "print(\"\\nğŸ“… With date components:\")\n",
    "print(full_data[['Date', 'Month', 'DayOfWeek', 'Week']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ‹ï¸ STEP 3: Selection and Filtering\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Let's answer some business questions using selection techniques.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” STEP 3: DATA SELECTION & FILTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Q1: High-value transactions (Net Revenue > $1000)\n",
    "print(\"\\n1ï¸âƒ£ High-value transactions (Net Revenue > $1000):\")\n",
    "high_value = full_data[full_data['Net_Revenue'] > 1000]\n",
    "print(f\"   Found {len(high_value)} transactions\")\n",
    "print(high_value[['ProductName', 'StoreName', 'Quantity', 'Net_Revenue']].head())\n",
    "\n",
    "# Q2: Electronics sold in Central region stores\n",
    "print(\"\\n2ï¸âƒ£ Electronics in Central region:\")\n",
    "central_electronics = full_data[\n",
    "    (full_data['Category'] == 'Electronics') & \n",
    "    (full_data['Region'] == 'Central')\n",
    "]\n",
    "print(f\"   Found {len(central_electronics)} transactions\")\n",
    "print(central_electronics[['ProductName', 'StoreName', 'Net_Revenue']].head())\n",
    "\n",
    "# Q3: Transactions with discounts over 10%\n",
    "print(\"\\n3ï¸âƒ£ High discount transactions (>10%):\")\n",
    "high_discount = full_data.query(\"Discount > 0.10\")\n",
    "print(f\"   Found {len(high_discount)} transactions\")\n",
    "print(f\"   Total discount given: ${high_discount['Discount_Amount'].sum():,.2f}\")\n",
    "\n",
    "# Q4: Weekend sales\n",
    "print(\"\\n4ï¸âƒ£ Weekend sales:\")\n",
    "weekends = full_data[full_data['DayOfWeek'].isin(['Saturday', 'Sunday'])]\n",
    "print(f\"   Weekend transactions: {len(weekends)}\")\n",
    "print(f\"   Weekend revenue: ${weekends['Net_Revenue'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80938c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ‹ï¸ STEP 4: Data Aggregation (Business Intelligence)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Create executive-level summary reports using aggregation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š STEP 4: DATA AGGREGATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summary 1: Performance by Store\n",
    "print(\"\\n1ï¸âƒ£ STORE PERFORMANCE REPORT:\")\n",
    "store_performance = full_data.groupby(['StoreName', 'ManagerName']).agg(\n",
    "    total_revenue=('Net_Revenue', 'sum'),\n",
    "    avg_transaction=('Net_Revenue', 'mean'),\n",
    "    num_transactions=('TransactionID', 'count'),\n",
    "    total_units=('Quantity', 'sum')\n",
    ").round(2).sort_values('total_revenue', ascending=False)\n",
    "print(store_performance)\n",
    "\n",
    "# Summary 2: Performance by Category\n",
    "print(\"\\n2ï¸âƒ£ CATEGORY PERFORMANCE REPORT:\")\n",
    "category_performance = full_data.groupby('Category').agg(\n",
    "    total_revenue=('Net_Revenue', 'sum'),\n",
    "    avg_price=('UnitPrice', 'mean'),\n",
    "    total_units=('Quantity', 'sum'),\n",
    "    discount_impact=('Discount_Amount', 'sum')\n",
    ").round(2).sort_values('total_revenue', ascending=False)\n",
    "print(category_performance)\n",
    "\n",
    "# Summary 3: Regional Performance\n",
    "print(\"\\n3ï¸âƒ£ REGIONAL PERFORMANCE REPORT:\")\n",
    "regional_performance = full_data.groupby('Region').agg(\n",
    "    total_revenue=('Net_Revenue', 'sum'),\n",
    "    avg_revenue_per_sale=('Net_Revenue', 'mean'),\n",
    "    num_transactions=('TransactionID', 'count')\n",
    ").round(2).sort_values('total_revenue', ascending=False)\n",
    "print(regional_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ‹ï¸ STEP 5: Executive Dashboard Visualizations\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Create a comprehensive dashboard for executive presentation.\n",
    "\"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(16, 14))\n",
    "\n",
    "# 1. Revenue by Store (Top Left)\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "store_rev = full_data.groupby('StoreName')['Net_Revenue'].sum().sort_values(ascending=True)\n",
    "colors = sns.color_palette(\"viridis\", len(store_rev))\n",
    "bars = ax1.barh(store_rev.index, store_rev.values, color=colors)\n",
    "ax1.set_title('Revenue by Store', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Net Revenue ($)')\n",
    "ax1.bar_label(bars, fmt='$%.0f', padding=3)\n",
    "\n",
    "# 2. Revenue by Category (Top Middle)\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "cat_rev = full_data.groupby('Category')['Net_Revenue'].sum()\n",
    "colors = sns.color_palette(\"husl\", len(cat_rev))\n",
    "ax2.pie(cat_rev.values, labels=cat_rev.index, autopct='%1.1f%%', \n",
    "        colors=colors, explode=[0.03]*len(cat_rev))\n",
    "ax2.set_title('Revenue Distribution by Category', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Top 10 Products by Revenue (Top Right)\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "product_rev = full_data.groupby('ProductName')['Net_Revenue'].sum().nlargest(10)\n",
    "sns.barplot(x=product_rev.values, y=product_rev.index, palette='viridis', ax=ax3)\n",
    "ax3.set_title('Top 10 Products by Revenue', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Net Revenue ($)')\n",
    "\n",
    "# 4. Daily Revenue Trend (Bottom Left - spans 2 columns)\n",
    "ax4 = fig.add_subplot(2, 3, (4, 5))\n",
    "daily_rev = full_data.groupby(full_data['Date'].dt.date)['Net_Revenue'].sum()\n",
    "ax4.plot(daily_rev.index, daily_rev.values, color='#3498db', linewidth=1.5, alpha=0.7)\n",
    "ax4.fill_between(daily_rev.index, daily_rev.values, alpha=0.3, color='#3498db')\n",
    "ax4.set_title('Daily Revenue Trend', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Net Revenue ($)')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Revenue by Day of Week (Bottom Right)\n",
    "ax5 = fig.add_subplot(2, 3, 6)\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_rev = full_data.groupby('DayOfWeek')['Net_Revenue'].mean().reindex(day_order)\n",
    "colors = ['#e74c3c' if d in ['Saturday', 'Sunday'] else '#3498db' for d in day_order]\n",
    "bars = ax5.bar(range(len(day_order)), dow_rev.values, color=colors)\n",
    "ax5.set_xticks(range(len(day_order)))\n",
    "ax5.set_xticklabels([d[:3] for d in day_order])\n",
    "ax5.set_title('Average Revenue by Day of Week', fontsize=12, fontweight='bold')\n",
    "ax5.set_ylabel('Average Revenue ($)')\n",
    "\n",
    "plt.suptitle('ğŸ“Š SALES EXECUTIVE DASHBOARD', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“ˆ KEY METRICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total Revenue:        ${full_data['Net_Revenue'].sum():,.2f}\")\n",
    "print(f\"Total Transactions:   {len(full_data):,}\")\n",
    "print(f\"Average Transaction:  ${full_data['Net_Revenue'].mean():,.2f}\")\n",
    "print(f\"Total Discounts:      ${full_data['Discount_Amount'].sum():,.2f}\")\n",
    "print(f\"Units Sold:           {full_data['Quantity'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b98186",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Section 7: Summary & Assignment Preview\n",
    "\n",
    "### ğŸ“ Key Takeaways\n",
    "\n",
    "| Concept | Key Points |\n",
    "|---------|------------|\n",
    "| **Selection** | `.loc[]` = labels (inclusive), `.iloc[]` = positions (exclusive), Boolean masks for filtering |\n",
    "| **Aggregation** | `groupby()` â†’ split-apply-combine, `.agg()` for multiple functions |\n",
    "| **Merging** | `inner` = intersection, `left`/`right` = keep one side, `outer` = union |\n",
    "| **Transformation** | Use operators for math, `.map()` for element-wise, `.apply()` for row-wise |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”‘ Common Pitfalls to Avoid\n",
    "\n",
    "1. **`.loc[]` vs `.iloc[]`** - Remember: loc is inclusive, iloc excludes the endpoint\n",
    "2. **Boolean conditions** - Use `&`/`|` (not `and`/`or`) with parentheses!\n",
    "3. **String operations** - Always use `.str.` accessor for string methods\n",
    "4. **Join types** - Choose carefully: inner loses data, outer creates NaNs\n",
    "5. **Performance** - Prefer vectorized operations over `.apply()` loops\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82e4ce",
   "metadata": {},
   "source": [
    "### ğŸ  Assignment Preview: Kaggle Football Dataset\n",
    "\n",
    "This week's assignment introduces **Kaggle Notebooks** â€“ they work just like Jupyter notebooks!\n",
    "\n",
    "**What you'll do:**\n",
    "1. Set up a Kaggle account and create a new notebook\n",
    "2. Work with an international football dataset\n",
    "3. Apply all the techniques from today:\n",
    "   - Select specific columns and filter rows\n",
    "   - Aggregate statistics by team\n",
    "   - Merge home and away game perspectives\n",
    "   - Transform data to calculate win/loss records\n",
    "\n",
    "**The Big Challenge (Task 9):**\n",
    "You'll reorganize data so each row represents one team's perspective of a game. This involves:\n",
    "1. Selecting relevant columns\n",
    "2. Renaming columns for home team perspective\n",
    "3. Renaming columns for away team perspective\n",
    "4. Concatenating both perspectives together\n",
    "5. Grouping by team to find average points against\n",
    "\n",
    "**Tips for Success:**\n",
    "- Print after each step to verify your work\n",
    "- Break complex problems into smaller steps\n",
    "- Remember: Python is Python whether in VS Code, Jupyter, or Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸˆ BONUS: Mini Football Dataset Practice\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Here's a simplified version of the assignment's logic.\n",
    "Practice this pattern before tackling the full assignment!\n",
    "\"\"\"\n",
    "\n",
    "# Simulated football data\n",
    "football_games = pd.DataFrame({\n",
    "    'home_team': ['USA', 'Brazil', 'Germany', 'USA', 'Brazil'],\n",
    "    'away_team': ['Brazil', 'Germany', 'USA', 'Germany', 'USA'],\n",
    "    'home_score': [2, 3, 1, 0, 2],\n",
    "    'away_score': [1, 1, 2, 0, 2]\n",
    "})\n",
    "\n",
    "print(\"Original Football Data:\")\n",
    "print(football_games)\n",
    "\n",
    "# Step 1: Create home team perspective\n",
    "home_perspective = football_games.rename(columns={\n",
    "    'home_team': 'team',\n",
    "    'away_team': 'opponent',\n",
    "    'home_score': 'goals_for',\n",
    "    'away_score': 'goals_against'\n",
    "})\n",
    "\n",
    "print(\"\\nHome Team Perspective:\")\n",
    "print(home_perspective)\n",
    "\n",
    "# Step 2: Create away team perspective\n",
    "away_perspective = football_games.rename(columns={\n",
    "    'away_team': 'team',\n",
    "    'home_team': 'opponent',\n",
    "    'away_score': 'goals_for',\n",
    "    'home_score': 'goals_against'\n",
    "})\n",
    "\n",
    "print(\"\\nAway Team Perspective:\")\n",
    "print(away_perspective)\n",
    "\n",
    "# Step 3: Combine both perspectives\n",
    "all_games = pd.concat([home_perspective, away_perspective], ignore_index=True)\n",
    "\n",
    "print(\"\\nCombined (All Games from Each Team's Perspective):\")\n",
    "print(all_games)\n",
    "\n",
    "# Step 4: Aggregate by team\n",
    "team_stats = all_games.groupby('team').agg(\n",
    "    games_played=('opponent', 'count'),\n",
    "    total_goals_for=('goals_for', 'sum'),\n",
    "    total_goals_against=('goals_against', 'sum'),\n",
    "    avg_goals_for=('goals_for', 'mean'),\n",
    "    avg_goals_against=('goals_against', 'mean')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Team Statistics:\")\n",
    "print(team_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b6f4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“‹ Quick Reference Cheat Sheet\n",
    "\n",
    "```python\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DATA SELECTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "df['column']                        # Single column (Series)\n",
    "df[['col1', 'col2']]               # Multiple columns (DataFrame)\n",
    "df.loc[0:2]                         # Label-based (INCLUSIVE)\n",
    "df.iloc[:2]                         # Position-based (EXCLUSIVE)\n",
    "df[df['col'] > 5]                   # Boolean filter\n",
    "df[(cond1) & (cond2)]              # Multiple conditions (use & | ~)\n",
    "df.query(\"col > 5 and col2 == 'A'\") # SQL-like filtering\n",
    "df[df['col'].isin([1, 2, 3])]      # Check membership\n",
    "df['col'].str.contains('pattern')  # String operations\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DATA AGGREGATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "df.groupby('col').sum()             # Simple aggregation\n",
    "df.groupby('col')['val'].mean()     # Aggregate one column\n",
    "df.groupby(['c1', 'c2']).agg({      # Multiple aggregations\n",
    "    'val1': ['sum', 'mean'],\n",
    "    'val2': 'count'\n",
    "})\n",
    "df.groupby('col').agg(              # Named aggregations\n",
    "    total=('val', 'sum'),\n",
    "    average=('val', 'mean')\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MERGING & CONCATENATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "pd.merge(df1, df2, on='key')        # Default inner join\n",
    "pd.merge(df1, df2, how='left')      # Left join\n",
    "pd.merge(df1, df2, how='outer')     # Outer join\n",
    "pd.merge(df1, df2, left_on='a', right_on='b')  # Different key names\n",
    "pd.concat([df1, df2], ignore_index=True)  # Stack vertically\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DATA TRANSFORMATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "df['new'] = df['old'] * 2           # Arithmetic operations\n",
    "df['cat'] = df['val'].map({'a': 1}) # Value mapping\n",
    "df['cat'] = df['val'].map(func)     # Apply function element-wise\n",
    "df['new'] = df.apply(func, axis=1)  # Apply to each row\n",
    "np.where(cond, if_true, if_false)   # Vectorized if/else\n",
    "df.rename(columns={'old': 'new'})   # Rename columns\n",
    "df.drop('col', axis=1)              # Drop column\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a45b92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â“ Practice Questions for Discussion\n",
    "\n",
    "Test your understanding with these questions:\n",
    "\n",
    "### Selection Questions\n",
    "1. How would you select all rows where `Age > 30 AND Score > 80`?\n",
    "2. What's returned when you do `df['Name']` vs `df[['Name']]`?\n",
    "3. What's wrong with: `df[df['Age'] > 30 and df['Score'] > 80]`?\n",
    "\n",
    "### Aggregation Questions\n",
    "1. If you groupby 'Category', what does `.mean()` calculate?\n",
    "2. What's the difference between `agg('sum')` and `agg(['sum', 'mean'])`?\n",
    "3. How would you find the category with the highest total sales?\n",
    "\n",
    "### Merging Questions\n",
    "1. Which join type keeps all customers even if they haven't ordered?\n",
    "2. When do you get NaN values in a merge result?\n",
    "3. What's the difference between `merge()` and `concat()`?\n",
    "\n",
    "### Transformation Questions\n",
    "1. How do you add 10% to all values in the 'Price' column?\n",
    "2. When would you use `.map()` vs a simple operator?\n",
    "3. Why is `df['Name'].upper()` wrong?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've completed a comprehensive introduction to:\n",
    "- âœ… Data Selection with loc, iloc, and boolean filtering\n",
    "- âœ… Data Aggregation with groupby and agg\n",
    "- âœ… Merging DataFrames with different join types\n",
    "- âœ… Data Transformation with operators, map, and apply\n",
    "- âœ… Creating professional visualizations with matplotlib and seaborn\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete the Kaggle assignment\n",
    "2. Practice with your own datasets\n",
    "3. Explore more advanced Pandas features\n",
    "\n",
    "**Remember:** The best way to learn data wrangling is through practice. Don't be afraid to experiment!\n",
    "\n",
    "---\n",
    "\n",
    "*ğŸ“§ Questions? Reach out during office hours or on Discord!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
